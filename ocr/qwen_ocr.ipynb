{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR-Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### qwen2.5vl:3b-Versuch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speicherbedarf: 4GB auf Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verarbeite Bild: thunberg_reisen02_0042.jpg\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ImageDescription\n  Invalid JSON: EOF while parsing a string at line 2 column 63 [type=json_invalid, input_value='{\\n  \"text_content\": \"! ...99999999999999999999999', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m\n\u001b[1;32m     25\u001b[0m response \u001b[38;5;241m=\u001b[39m chat(\n\u001b[1;32m     26\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqwen2.5vl:3b\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mImageDescription\u001b[38;5;241m.\u001b[39mmodel_json_schema(),  \u001b[38;5;66;03m# Pass in the schema for the response\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m},  \u001b[38;5;66;03m# Set temperature to 0 for more deterministic output\u001b[39;00m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Validierung und Ergebnisverarbeitung\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m image_description \u001b[38;5;241m=\u001b[39m \u001b[43mImageDescription\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m text_content \u001b[38;5;241m=\u001b[39m image_description\u001b[38;5;241m.\u001b[39mtext_content\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Speichere das Ergebnis in einer .txt-Datei\u001b[39;00m\n",
      "File \u001b[0;32m~/program/anaconda3/envs/nlp/lib/python3.9/site-packages/pydantic/main.py:656\u001b[0m, in \u001b[0;36mBaseModel.model_validate_json\u001b[0;34m(cls, json_data, strict, context)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    655\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ImageDescription\n  Invalid JSON: EOF while parsing a string at line 2 column 63 [type=json_invalid, input_value='{\\n  \"text_content\": \"! ...99999999999999999999999', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ollama import chat\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ImageDescription(BaseModel):\n",
    "    text_content: str\n",
    "\n",
    "# Ordner mit den Eingabebildern (relativer Pfad)\n",
    "input_folder = './ocr_images/'\n",
    "\n",
    "# Ordner für die Ausgabedateien erstellen (relativer Pfad)\n",
    "output_folder = './qwen3b_results_linebreak2/'\n",
    "os.makedirs(output_folder, exist_ok=True)  \n",
    "\n",
    "# Iteriere durch alle Bilddateien im Eingabeordner\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # Nur Bilddateien berücksichtigen\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Meldung, welches Bild gerade verarbeitet wird\n",
    "        print(f\"Verarbeite Bild: {filename}\")\n",
    "        \n",
    "        # Anfrage an das Modell\n",
    "        response = chat(\n",
    "            model='qwen2.5vl:3b',\n",
    "            format=ImageDescription.model_json_schema(),  # Pass in the schema for the response\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': 'Extrahiere den vollständigen Text aus dem Bild. Achte auf Zeilenumbrüche.',\n",
    "                    'images': [image_path],\n",
    "                },\n",
    "            ],\n",
    "            options={'temperature': 0},  # Set temperature to 0 for more deterministic output\n",
    "        )\n",
    "        \n",
    "        # Validierung und Ergebnisverarbeitung\n",
    "        image_description = ImageDescription.model_validate_json(response.message.content)\n",
    "        text_content = image_description.text_content\n",
    "        \n",
    "        # Speichere das Ergebnis in einer .txt-Datei\n",
    "        output_file = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(text_content)\n",
    "        \n",
    "        print(f\"Ergebnis für {filename} gespeichert in {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### qwen2.5vl:7b-Versuch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speicherbedarf: 6,5 GB auf Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ollama import chat\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ImageDescription(BaseModel):\n",
    "    text_content: str\n",
    "\n",
    "# Ordner mit den Eingabebildern (relativer Pfad)\n",
    "input_folder = './ocr_images/'\n",
    "\n",
    "# Ordner für die Ausgabedateien erstellen (relativer Pfad)\n",
    "output_folder = './qwen7b_2results/'\n",
    "os.makedirs(output_folder, exist_ok=True)  \n",
    "\n",
    "# Iteriere durch alle Bilddateien im Eingabeordner\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # Nur Bilddateien berücksichtigen\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Meldung, welches Bild gerade verarbeitet wird\n",
    "        print(f\"Verarbeite Bild: {filename}\")\n",
    "        \n",
    "        # Anfrage an das Modell\n",
    "        response = chat(\n",
    "            model='qwen2.5vl:7b',\n",
    "            format=ImageDescription.model_json_schema(),  # Pass in the schema for the response\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': 'Extrahiere den vollständigen Text aus dem Bild.',\n",
    "                    'images': [image_path],\n",
    "                },\n",
    "            ],\n",
    "            options={'temperature': 0},  # Set temperature to 0 for more deterministic output\n",
    "        )\n",
    "        \n",
    "        # Validierung und Ergebnisverarbeitung\n",
    "        image_description = ImageDescription.model_validate_json(response.message.content)\n",
    "        text_content = image_description.text_content\n",
    "        \n",
    "        # Speichere das Ergebnis in einer .txt-Datei\n",
    "        output_file = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(text_content)\n",
    "        \n",
    "        print(f\"Ergebnis für {filename} gespeichert in {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gemma3:12b-it-qat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei ca. 12 GB Speichernutzung von Ollama hat sich das Modell dasaufgehängt. Gemma3:12b nutzt ca. 9GB und hängt sich ebenfalls auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ollama import chat\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ImageDescription(BaseModel):\n",
    "    text_content: str\n",
    "\n",
    "# Ordner mit den Eingabebildern (relativer Pfad)\n",
    "input_folder = './ocr_images/'\n",
    "\n",
    "# Ordner für die Ausgabedateien erstellen (relativer Pfad)\n",
    "output_folder = './gemma3_12b_qat_results/'\n",
    "os.makedirs(output_folder, exist_ok=True)  \n",
    "\n",
    "# Iteriere durch alle Bilddateien im Eingabeordner\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # Nur Bilddateien berücksichtigen\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Meldung, welches Bild gerade verarbeitet wird\n",
    "        print(f\"Verarbeite Bild: {filename}\")\n",
    "        \n",
    "        # Anfrage an das Modell\n",
    "        response = chat(\n",
    "            model='gemma3:12b',\n",
    "            format=ImageDescription.model_json_schema(),  # Pass in the schema for the response\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': 'Extrahiere den vollständigen Text aus dem Bild.',\n",
    "                    'images': [image_path],\n",
    "                },\n",
    "            ],\n",
    "            options={'temperature': 0},  # Set temperature to 0 for more deterministic output\n",
    "        )\n",
    "        \n",
    "        # Validierung und Ergebnisverarbeitung\n",
    "        image_description = ImageDescription.model_validate_json(response.message.content)\n",
    "        text_content = image_description.text_content\n",
    "        \n",
    "        # Speichere das Ergebnis in einer .txt-Datei\n",
    "        output_file = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(text_content)\n",
    "        \n",
    "        print(f\"Ergebnis für {filename} gespeichert in {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seite 251 kann auch mit qwen2.5vl:3b nicht abgeschlossen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ImageDescription(BaseModel):\n",
    "\n",
    "  text_content: str\n",
    "\n",
    "path = '/Users/paulzimmermann/Library/Mobile Documents/com~apple~CloudDocs/Studium/tu/Master/MA/second_try/scripts/ollama/1.ocr_correction/thunberg_reisen02_0251.jpg'\n",
    "\n",
    "output_folder = './qwen3b_results/'\n",
    "os.makedirs(output_folder, exist_ok=True)  \n",
    "\n",
    "response = chat(\n",
    "  model='qwen2.5vl:3b',\n",
    "  format=ImageDescription.model_json_schema(),  # Pass in the schema for the response\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'Extrahiere den vollständigen Text aus dem Bild.',\n",
    "      'images': [path],\n",
    "    },\n",
    "  ],\n",
    "  options={'temperature': 0},  # Set temperature to 0 for more deterministic output\n",
    ")\n",
    "\n",
    "image_description = ImageDescription.model_validate_json(response.message.content)\n",
    "print(image_description)\n",
    "\n",
    "output_file = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "  f.write(text_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR-Korrektur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QWEN auf Bilderkennung spezialisiert. Daher der Versuch eine Textkorrektur mit QWEN überflüssig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemma3:12b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ollama import chat\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ImageDescription(BaseModel):\n",
    "    text_content: str\n",
    "\n",
    "# Ordner mit den Eingabebildern (relativer Pfad)\n",
    "input_folder = './qwen3b_results_linebreak/'\n",
    "\n",
    "# Ordner für die Ausgabedateien erstellen (relativer Pfad)\n",
    "output_folder = './qwent_lb-gemma3_12b_correction1/'\n",
    "os.makedirs(output_folder, exist_ok=True)  \n",
    "\n",
    "# Iteriere durch alle Bilddateien im Eingabeordner\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # Nur Bilddateien berücksichtigen\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Meldung, welches Bild gerade verarbeitet wird\n",
    "        print(f\"Verarbeite Bild: {filename}\")\n",
    "        \n",
    "        # Anfrage an das Modell\n",
    "        response = chat(\n",
    "            model='gemma3:12b',\n",
    "            format=ImageDescription.model_json_schema(),  # Pass in the schema for the response\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': 'Korrigiere die OCR-Fehler in dem Text',\n",
    "                },\n",
    "            ],\n",
    "            options={'temperature': 0},  # Set temperature to 0 for more deterministic output\n",
    "        )\n",
    "        \n",
    "        # Validierung und Ergebnisverarbeitung\n",
    "        image_description = ImageDescription.model_validate_json(response.message.content)\n",
    "        text_content = image_description.text_content\n",
    "        \n",
    "        # Speichere das Ergebnis in einer .txt-Datei\n",
    "        output_file = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(text_content)\n",
    "        \n",
    "        print(f\"Ergebnis für {filename} gespeichert in {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama3.2:3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei Korrektur aufgehängt (250)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verarbeite Datei: thunberg_reisen02_0035.txt\n",
      "Korrigiertes Ergebnis für thunberg_reisen02_0035.txt gespeichert in ./qwent_lb-gemma3_12b_correction1/thunberg_reisen02_0035.txt\n",
      "Verarbeite Datei: thunberg_reisen02_0023.txt\n",
      "Korrigiertes Ergebnis für thunberg_reisen02_0023.txt gespeichert in ./qwent_lb-gemma3_12b_correction1/thunberg_reisen02_0023.txt\n",
      "Verarbeite Datei: thunberg_reisen02_0025.txt\n",
      "Korrigiertes Ergebnis für thunberg_reisen02_0025.txt gespeichert in ./qwent_lb-gemma3_12b_correction1/thunberg_reisen02_0025.txt\n",
      "Verarbeite Datei: thunberg_reisen02_0031.txt\n",
      "Korrigiertes Ergebnis für thunberg_reisen02_0031.txt gespeichert in ./qwent_lb-gemma3_12b_correction1/thunberg_reisen02_0031.txt\n",
      "Verarbeite Datei: thunberg_reisen02_0019.txt\n",
      "Korrigiertes Ergebnis für thunberg_reisen02_0019.txt gespeichert in ./qwent_lb-gemma3_12b_correction1/thunberg_reisen02_0019.txt\n",
      "Verarbeite Datei: thunberg_reisen02_0042.txt\n",
      "Korrigiertes Ergebnis für thunberg_reisen02_0042.txt gespeichert in ./qwent_lb-gemma3_12b_correction1/thunberg_reisen02_0042.txt\n",
      "Verarbeite Datei: thunberg_reisen02_0250.txt\n",
      "Korrigiertes Ergebnis für thunberg_reisen02_0250.txt gespeichert in ./qwent_lb-gemma3_12b_correction1/thunberg_reisen02_0250.txt\n",
      "Verarbeite Datei: thunberg_reisen02_0017.txt\n",
      "Korrigiertes Ergebnis für thunberg_reisen02_0017.txt gespeichert in ./qwent_lb-gemma3_12b_correction1/thunberg_reisen02_0017.txt\n",
      "Verarbeite Datei: thunberg_reisen02_0007.txt\n",
      "Korrigiertes Ergebnis für thunberg_reisen02_0007.txt gespeichert in ./qwent_lb-gemma3_12b_correction1/thunberg_reisen02_0007.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ollama import chat\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class TextCorrection(BaseModel):\n",
    "    corrected_text: str\n",
    "\n",
    "# Ordner mit den Eingabe-.txt-Dateien (relativer Pfad)\n",
    "input_folder = './qwen3b_results_linebreak/'\n",
    "\n",
    "# Ordner für die Ausgabedateien erstellen (relativer Pfad)\n",
    "output_folder = './qwent_lb-gemma3_12b_correction1/'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iteriere durch alle .txt-Dateien im Eingabeordner\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.lower().endswith('.txt'):  # Nur .txt-Dateien berücksichtigen\n",
    "        input_file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Meldung, welche Datei gerade verarbeitet wird\n",
    "        print(f\"Verarbeite Datei: {filename}\")\n",
    "        \n",
    "        # Lese den Textinhalt der .txt-Datei\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "            text_content = f.read()\n",
    "        \n",
    "        # Anfrage an das Modell zur Korrektur des Textes\n",
    "        response = chat(\n",
    "            model='gemma3:12b',\n",
    "            format=TextCorrection.model_json_schema(),  # Pass in the schema for the response\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': 'Korrigiere die OCR-Fehler in dem folgenden Text:',\n",
    "                },\n",
    "                {\n",
    "                    'role': 'assistant',\n",
    "                    'content': text_content,\n",
    "                },\n",
    "            ],\n",
    "            options={'temperature': 0},  # Set temperature to 0 for more deterministic output\n",
    "        )\n",
    "        \n",
    "        # Validierung und Ergebnisverarbeitung\n",
    "        corrected_text = TextCorrection.model_validate_json(response.message.content).corrected_text\n",
    "        \n",
    "        # Speichere das korrigierte Ergebnis in einer neuen .txt-Datei\n",
    "        output_file_path = os.path.join(output_folder, filename)\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(corrected_text)\n",
    "        \n",
    "        print(f\"Korrigiertes Ergebnis für {filename} gespeichert in {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
